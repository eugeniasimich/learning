#+TITLE: Distributed Systems
#+OPTIONS: num:nil
#+OPTIONS: toc:nil
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup

| These are notes on CSE138 course on distributed systems [fn:ytlink]

#+TOC: headlines 2

* Intro


#+BEGIN_QUOTE
A distributed system is one in which the failure of a computer you didn’t even know existed can render your own computer unusable.  
-- Leslie Lamport
 #+END_QUOTE


Definition of distributed systems: Partial failure + unbounded latency

Why to deal with this?
- to make things faster, using more computers
- to have more data than what it can fit on one machine
- reliability (by redundancy, replicas)
- throughput (by proximity)

** Network models

+ Synchronous: A synch network is one where there exists an ~n~ such that no messages take longer than ~n~ units of time to be delivered.
+ Asynchronous: An asynch network is one where there exists NO ~n~ such that no messages take longer than ~n~ units of time to be delivered.

We will focus on asych, as it is the most realistic. 



* Clocks

** Physical clocks: Monotonic vs time-of-day clocks.
+ Monotonic: always go forwads. Are meaningful only inside the machine (they are computed as milliseconds since you restarted the machine or something like that). Good for mesuring /durations/.
+ Time-of-day: Are normally synched agains NTP (network time protocol). Can jump forward and backwards. Not very good for mesuring durations. Better for timestamps. 


#+BEGIN_SRC
   __         __ 
  |m1| x=5   |m2| x=5 
   |           |
3pm| \________ |
   |    x++   \| x=6
   |           |
   |       3pm |
   |           |
   v           v 
#+END_SRC
[Example 1] Example where neither of these physical clocks are useful


As ~m1~ and ~m2~ have their clocks a little bit unsynchronized, if we asked them to take a snapshot at 3pm, then ~m1~ would have said ~x=5~ while ~m2~ would have said ~x=6~

** Logical clocks

Only measures the order of events. 

A->B : A happened before B

This means:
- A could have caused B
- B could not have caused B

Useful for debugging.

We say that:
+ Consistent with causality: ~A->B~ => Logical clock of ~A~ < Logical clock of ~B~
+ Characterizes causality: Logical clock of ~A~ < Logical clock of ~B~ => ~A->B~

*** Lamport diagrams

Spacetime diagrams

Given events ~A~ and ~B~, we say ~A->B~ (~A~ happens before ~B~) if 
1. ~A~ and ~B~ happens in the same process line with ~A~ before ~B~.
2. ~A~ is a send event and ~B~ is the corresponding recieve event.
3. if ~A->C~ and ~C->B~ then ~A->B~ (transitivity)

#+BEGIN_SRC
   __         __ 
  |m1|       |m2|
   |           |
 A |\_________ |
   |          \|
   |           | B
   |           |
 D |           | C
   v           v
#+END_SRC
[Example 2] 

In the example, we know
1. ~A->B~ due to 2.
2. ~B->C~ due to 1.
3. ~A->D~ due to 1.
4. ~A->C~ due to our first two items and 3.
5. Neither ~B->D~ or ~D->B~ are true. So we say ~B~ and ~D~ are concurrent: ~B||D~


**** Causal anomaly 


Alice sends the message "Bob smells" to both Bob and Carol. Bob receives it first and answers "Fuck you Alice" to both Alice and Carol. Carol receives "Fuck you Alice" first and then "Bob Smells"

#+BEGIN_SRC
           _____         ___                _____
          |Alice|       |Bob|              |Carol|
             |              |                 |
 "Bob smells"|------------->|                 |
             |\             |                 |
             | \            |                 |
             |  \           | "Fuck you       |
             |   \          |  Alice"         |
             |    \        /|\--------------->|
             |     \     _/ |                 |
             |      \---/-------------------->|  ??
             |         /    |                 |
             |        /     |                 |
             | <-----/      |                 |
             v              v                 v
#+END_SRC
[Example 3]

This is known as /causal anomality/.



** State and events

We can look at the event log and reconstruct state. Not the other way around.

#+BEGIN_SRC
      __         __ 
     |m1|       |m2| x=5 
      |            | A
"set x|---------   |
 to 6"|         \->| B  
      |            | x=6    
"set x|---------   |
 to 7"|         \->| C  
      |            | x=7    
      v            v
#+END_SRC
[Example 4]

| log             |
|----------------|
| A    ??         |
| B   set x to 6 |
| C   set x to 7 |


** Partial order

Is "~->~" relation a partial order. Almost. It lacks reflexivity. So it's an /irreflective partial order/ (also know as /strict partial order/).



** Lamport clocks

The lamport clock of an event ~A~ (written ~LC(A)~

"clock condition": if ~A->B~ then ~LC(A) < LC(B)~ (consistent with causality)

Lamport clock algoritm:

1) Every process keeps a counter
2) On each event on a process, the counter is incremented. 
3) When sending a message, the sender includes it's counter in the message.
4) When receiveing a message, the receiver sets its counter to be ~max(local_counter, received_counter) + 1~

We can note that following these rules, ~LC(A)<LC(B)~ does not imply ~A->B~ so lamport clocks are consistent with causality but do not characterize causality.

But, by contrapositive: ~!(LC(A)<LC(B)) => ! A->B~, in other words ~LC(A) >= LC(B) =>  A||B~

There's a logical clock with both characteristics..

** Vector clocks

1) Every process keeps a vector of integers initialized at 0s. This vector has dimension ~n~, for ~n~ processes.
2) On every event, a process increments its position in its vector clock.
3) When sending a message, the sender includes its vector clock in the message (after the increment from step 2., because sends are events.)
4) When receiving a message, the receiver will update its vector clock to ~max(local_vector, received_vector)~ and increment its own position, because receive is an event. We define ~max~ to be the pointwise max.



#+BEGIN_SRC 
           _____           ___                _____
          |Alice|         |Bob|              |Carol|
    [0,0,0]  |       [0,0,0]  |         [0,0,0] |
             |                |                 |
             |                |                 |
             |                |     ----------- + [0,0,1]
             |        [0,1,1] +<---/            |
             |                |                 |
             |     -----------+ [0,2,1]         |
    [1,2,1]  +<---/           |                 |
             |                |                 |             
             |        [0,3,1] +\-------------   |
             |                |              \->+ [0,3,2]
             |                |                 |
    [2,2,1]  +-------------   |                 |
             |             \->+ [2,4,1]         |
             |                |                 + [0,3,3]
             |                +- [2,5,1]       /|
             |                | \__________   / |
             |                |            \-/->+ [2,5,4] 
             |                |             /   |
    [3,3,3]  +<----------------------------/    |
             |                |              __/+ [2,5,5] 
    [4,5,5]  +<-----------------------------/   |
             |                |                 |
             v                v                 v
#+END_SRC
[Example 5]

Each event has a /logical clock/ assigned, a /vector clock/. 

An there's a characteristic that we can notice:  ~A->B <=> VC(A) < VC(B)~

Given any event, there's a set of events whose vector clocks are /smaller/, some events whose vector clocks are /higher/, using point-wise comparision. There are also events not comparable using this comparison function.

For example, if we pick event ~[2,4,1]~, there is a set of events that have smaller vector clocks (~[0,0,1]~, ~[0,1,1]~, ect)
and some that are higher (~[2,5,5]~, ~[2,5,4]~, etc) and some not comparable (~[0,3,3]~, ~[3,3,3]~, etc)

Events whose vector clocks are not comparable, are concurrent, /causally independent/.

If we want to know the /happen-before/ relashionship, we can just compare vectors.


+ Protocol :: Set of rules that processes use to communicate to each other. 

* Delivery

** FIFO delivery

Ensures that the order in which messages are sent by a process is the same as the order in which they are delivered by other processes.

+ FIFO delivery :: If a process sends message ~m2~ after message ~m1~, any process delivering both, delivers ~m1~ first.

Sending: Something a process DO.
Receiving: Something that HAPPENS to a process .
Delivering: Something a process CAN DO with a message it receives. (Messages can be queued up and deliver posponed)

#+BEGIN_SRC
   __          __ 
  |A|          |B|
   |            |
   |\___m1_     |
   |-------\--->|
   |  m2    \   | 
   |         \->|
   |            | 
   v            v
#+END_SRC
[Example 6] 

/Example 6 violates FIFO delivery/

| FIFO delivery is already part of TCP

FIFO delivery can be implemented with sequence numbers: Messages are tagged with ~(senderID, sequenceNbr)~

Senders increment the sequence number after each send. If a received message has a SN equal to previously seen SN plus one, then deliver.

FIFO delivery only works well if /reliable delivery is guaranteed/. (TCP has both)



#+BEGIN_SRC
   __          __ 
  |A|          |B|
   |            |
   |\___m1_     |
   |       \--->|
   |            |
   |  ack   ----|
   |<------/    |
   |            |
   |\___m2_     |
   |       \--->|
   |            |
   |  ack   ----|
   |<------/    |
   v            v
#+END_SRC
[Example 7] 

A way of ensuring FIFO. Of couse this is slower.

** Causal delivery

 Ensures that messages with a causal relationship (i.e., one message happens before another) are delivered in the same order as they were sent.

+ Causal delivery :: If ~m1~'s send happens before ~ms~'s send, then ~m1~'s delivery must happen before ~m2~'s delivery.

/Example 6 of FIFO delivery violation, also violates causal delivery./

/Example 3 (Bob smells, FU Alice) violates causal delivery but not FIFO delivery/. Why? Because FIFO is about messages being sent /from the same sender/.

We can enforce causal delivery by using vector clocks

*** Causal broadcast

Causal broadcast is an algorithm that ensures causal delivery in a setting where all messages are broadcast messages.

We want to define a deliverability condition that will indicate whether a message has to be delivere or not upon receival.

The algorithm chosen is vector clock (with no receive increase). The deliverability condition will be tied to the vector contained in the message metaata. A message ~m~ is deliverable at a process ~p~ if


\begin{align*}               
VC(m)[k] &= VC(p)[k] + 1 & \mbox{ if $k$ is the sender} \\
VC(m)[k] &\leq VC(p)[k]     & \mbox{ otherwise}
\end{align*}

Where \(VC(m)\) is the vector clock comming the message, an \(VC(p)\) is the vector clock currently stored in the process.

Let's observe this in the example 3, but using vector clocks

#+BEGIN_SRC
             _             _                  _
            |A|           |B|                |C|
     [0,0,0] |      [0,0,0] |         [0,0,0] |
             |              |                 |
     [1,0,0] |------------->| [1,0,0]         |
             |\             |                 |
             | \            |                 |
             |  \           |                 |
             |   \          |  [1,1,0]        |
             |    \        /|\--------------->| X
             |     \     _/ |                 |
             |      \---/-------------------->| Y 
             |         /    |    [1,0,0]      |
             |        /     |                 |
             | <-----/      |                 |
             v              v                 v
#+END_SRC
[Example 8]

At point ~X~, process ~C~ receives the vector clock ~[1,1,0]~. It's own vector clock, at this point is ~[0,0,0]~, so 
the second condition is violated. \(VC(m)[0] = 1 \nleq 0\). So ~C~ shouldn't deliver the first message. Instead, it can be queued.

At point ~Y~, process ~C~ receives the vector clock ~[1,0,0]~. In this case both conditions are met and the message can be delivered. ~c~'s vector clock now changes to be ~[1,0,0]~. So now the message queued can also be delivered.



** Totally-ordered delivery

Ensures that messages are delivered in the same order on all processes. 

+ Totally-ordered delivery :: If a proccess /delivers/ ~m1~ and then ~m2~, then all processes /delivering/ both should deliver ~m1~ first.

#+BEGIN_SRC
   _            _          _            _
  |A|          |B|        |C|          |D|
   |            |          |            |
   |____m1_     |          |      _m2___|
   |  \    \--->|          |<----/    / |
   |   \        |     m1   |         /  |
   |    \----------------->|        /   |
   |            |          |    m2 /    |
   |            |<----------------/     |
   |            |          |            |
   v            v          v            v
#+END_SRC
[Example 9] 

~B~ delivers ~m1~ first and then ~m2~, ~C~ delivers ~m2~ first and then ~m1~. So there's a violation of totally-ordered delivery.

Use case scenario: Imagine ~B~ and ~C~ being replicas of a data base.


** Relation between the three delivery guarantees

#+begin_src 
┌─All──────────────────────┐
│  ┌──FIFO─────────┐       │
│  │               │       │
│  │    Causal─┐   │       │
│  │   │       │   │       │
│  │   │    ┌──┼───┼─────┐ │
│  │   └────┼──┘   │     │ │
│  │        │      │     │ │
│  └────────┼──────┘     │ │
│           │            │ │
│           └────────T.O.  │
└──────────────────────────┘
#+end_src

This means that 
+ There are executions with both causal and totally ordered delivery.
+ There are executions with both FIFO and totally ordered delivery, but not causal.
+ There are executions with totally ordered delivery buta not FIFO.


* Snapshots of distributed system

Taking a snapshot at certain point in time, using wall clock time, is not safe.

If we have two events ~A~ and ~B~, and ~A~ happens-before ~B~, if ~B~ is in the snapshot, then ~A~ also.

We do need snapshots for:
 + Checkpointing
 + Deadlock detection
 + Detection of any stable property (one it becomes true, it stays true)

**** Chandy-Lamport algorithm (1985)

We will call /channel/ to a connection between two processes. Communication between two processes go through a channel. We introduce this notion to ensure FIFO ordering. This algorithm also assumes that messages aren't lost, corrupted or duplicated. Also assumes processes don't crash.

/Initiator/: 
1. Records its own state. 
2. Sends a marker message out on all its outgoing channels.
3. Starts recording the messages it receives in all its incoming channels.

/When process p_i receives a marker message on C_{ki}/:
+ If it's the first marker it has seen (sent or received)
  1. p_i records its state
  2. p_i will mark channel C_{ki} as empty
  3. p_i sends a marker in all its outgoing channels
  4. p_i starts recording in all its incomming channels
+ Otherwise
  1. p_i stops recording on C_{ki}

Let's look at the following example[fn:chandy-lamport], where process ~P1~ is the snapshot initiator.

#+BEGIN_SRC
C21= [H->D]   C12= <>   C13= <>
C31= <>       C32= <>   C23= <>

   __           __          __      
  |P1|         |P2|        |P3|    
   |            |            |            
 A +            |            |
   |            |       -----+ I            
 B +_______     |      /     |     
   |       \--->+ F   /      |
   |===         |    /       |
   |  \\====================>|
   |   \\       |  /         |
   |    \\    G +<-          |    
   |     \\     |            |
 C +      \\    |        ====|
   |       \\   |      //    |
   |        ||  |     //     + J
   |<===========|======      |
   |        ||  |     ||     |
   |     -------+ H   ||     |
   |    /   ||  |     ||     |
   |   /    ||  |<=====      |
 D +<-/     ||  |            |
   |     =======|===========>|
   |   //   ||  |            |
   |<===     \\ |            |
   |          =>|            |
   |            |            |            
   v            v            v            
#+END_SRC
[Example 10] 


1. First, ~P1~ records its own state, which consists of events ~A~ and ~B~. Then it trigers two marking events (in the example marking events are represented with double lines). The marker message headed to ~P2~ is taking a while. The marker message sent to ~P3~ arrives quickly. Let's follow from there.
2. When ~A~'s marker message arrives at ~P3~, this process has never seen a marker message so far, so ~P3~ records its state, which consists of only the event ~I~ and marks channel ~C13~ as empty. Then it broadcasts marker messages and starts recording on all its incoming channels (~C23~).
3. ~P3~ has sent out its marker messages. Let’s consider the one that went to ~P1~ first. This message is not the first marker message that ~P1~ has seen (it has seen its own marker messages), so it will stop recording on ~C31~. As nothing happened on that channel, it ends up being empty.
4. Let's look at the marker message that ~P3~ sent to ~P2~. As this is the first marker message that ~P2~ has seen, then it first records it state: ~F~, ~G~ and ~H~. Then it will mark channel ~C32~ as empty and start recording on all its incoming channels (only ~C12~, as ~C32~ was already marked as empty). ~P2~ also sends its marker messages.
5. ~P2~ marker messages will finally arrive to ~P1~ and ~P3~, that will lead to channels ~C21~ and ~C23~ recordings being stopped. In the case of ~C21~, only one message was recorded, the message whose send event was ~H~ and whose receive event was ~D~. So that event goes into ~C21~'s final channel state. In the case of ~C23~, it ends up being empty.
6. Finally, the marker message from ~P1~ to ~P2~ arrives last, and it leads to channel ~C12~ recording being stopped while empty.

The final snapshot consists of events: ~A~, ~B~, ~F~, ~G~, ~H~ and ~I~. Also, the channels recordings are ~C21= [H->D]~ while the rest are empty.

A question that might arise is: What’s the point of recording channel states? 
Imagine that we want at most one process in this execution to have exclusive access to some sort of resource (say, a file). We can think of access to the resource as being a token that processes pass around. At any given time, either a process should have the token, or it should be in transit between processes.

If we hadn't recorded on channels, the token status would have been lost from the snapshot perspective.


+ Chandy-Lamport guarantees :: 
  + Consistecy with causality.
  + Termination.
  + Decentralization: Works fine if more than one process initiates.




* Fault models

** Safety and liveness

+ Safety :: A property that states that some "bad" thing /won't/ ever happen. Properties discussed so far (FIFO delivery, causal delivery, totally-ordered delivery) are safety properties.


+ Liveness :: A property that states that a "good" property /will/ eventually happen. An example of this is eventual deliver (reliable deliver), or eventual consistency.

Safety properties /can/ be violeted in a /finite/ execution. Liveness properties /cannot/ be violeted in a /finite/ execution.

All properties are either safety or liveness or a combination.

** Faults models

When designing a system, certain assumptions about the environment in which that system will operate need to be made. In particular, when designing a fault tolerant system, it's important to know which faults can happen. A fault model determines what kind of faults can occur. A *fault model* is a specification that indicates what kinds of faults a system can exhibit.

Let's take a look at the following example where processes ~m1~ and ~m2~ communicate with each other via the network.

#+begin_src ascii
  ┌─────────┐  
  │  x?     ▼  
┌─┴┐       ┌──┐
│m1│       │m2│
└──┘       └┬─┘
  ▲   5     │  
  └─────────┘  
#+end_src


Things that can go wrong from ~m1~'s perspective, and not hear the expected answer back:

  1. Message from ~m1~ can get lost
  2. Message from ~m1~ is slow
  3. ~m2~ crashed
  4. ~m2~ is slow
  5. Message from ~m2~ is slow
  6. Message from ~m2~ can get lost
  7. ~m2~ lies

*Crash fault*: A process fails by halting, it stops sending/receiving messages. Case 3\\
*Fail-stop fault*: A process fails by halting, and everyone knows it crashed (this is not the case in a crash fault). For example, a process can bradcast its death so everyone knows. It's not very realistic.
*Omission fault*: A message is lost, a process fails to send or receive a message. Cases 1 and 6\\
*Timing fault*: A process responds too late (or too early). Cases 2, 4 and 5\\
*Byzantine fault*: A process behaves in arbitrary way or even malicious. Case 7


Fail-stop faults \(\,\subset\,\) Crash faults \(\, \subset\, \) Omission faults \( \,\subset\, \) Byzantine faults 

Regarding timing faults, in a distributed model, those are expected.

** Two generals problem

Two armies, each led by a different general, are preparing to attack a fortified city. The armies are encamped near the city, each in its own valley. A third valley separates the two hills, and the only way for the two generals to communicate is by sending messengers through the valley. Unfortunately, the valley is occupied by the city's defenders and there's a chance that any given messenger sent through the valley will be captured.

#+BEGIN_SRC
             __          __ 
            |G1|        |G2|
Let's attack |           |
  at dawn    |\--------->|
             |           |
             |          /| Ok 
             |<--------- |
             |           | 
             v           v
#+END_SRC

General 1 sends a message proposing a combined attack at dawn. General 2 receives the message and responds positively. Can G1 confidently attack at dawn? No, as she knows that is impossible for G2 to know that her "Ok" message has arrived. Let's say G1 acknowledges G2's message. Can she be sure now? No. In this setting, it is impossible for G1 and G2 to attack being sure that the otther will.

The setting here is the *omission model*, where omission faults are expected.

A possible workaround is to base the attack on probability. G1 will send several messages to propose attacks and will stop sending them on G2's acknowledgement. If it's been a while since G2 has stopped listening to G1's messages, then it can be confident that she received the message. Confidence grows as time passes and no new messagges arrives.


#+BEGIN_SRC
             __          __ 
            |G1|        |G2|
Let's attack |           |
  at dawn    |\----->    |
Let's attack |           |
  at dawn    |\----->    |
Let's attack |           |
  at dawn    |\----->    |
Let's attack |           |
  at dawn    |\--------->|
Let's attack |           |
  at dawn    |\---->     |
             |          /| Ok 
             |<--------- | .
             |           | .
             |           | .
             |           | .
             v           v
#+END_SRC

A second workaround is based on common knowledge. Knowledge is considered *common* if everyone knows about it and everyone knows that everyone knows.


** Forms of fault tolerance

We've discussed byzantine faults, it turns out there are some of those faults that are easier to avoid.
Say that you can detect messages being corrupted (via checksum for example). In that case you can treat it as an ommision fault and just discard the message.

What does it mean to tolerate a class of faults? 
A correct program satisfies both its safety and liveness properties. 

How wrong does a program go in the presence of a given class of faults

|          | live        | not live  |
| safe     | masking     | fail safe |
| not safe | non masking |           |

For example, in FIFO delivery, let's say that its safety property talks about the order of the messages in the delivery, while liveness talks about messages eventually arriving. Then it might be useful to have a fails-safe model that preserves order while maybe compromising some messages arriving.

**** Reliable delivery 

Let ~p1~ be a process that delivers a message ~m~ to process ~p2~. Then if neither ~p1~ not ~p2~ crashes and not all messages are lost, then ~p2~ eventually delivers ~m~.


A possible implementation of reliable delivery is the one described in the two generals section. The sender might send the message on and on until it receives an ack. The problem with this implementation is that the receiver ack might be lost, so the receiver will still be getting messages. This might or might not be an issue. In the case of the two generals it was not an issue. In general, if the message is idempotent, there's not an issue.

Reliable delivery is also called /at-least-once/ delivery.

Can we have /exactly-once/ delivery?

Systems that claim to have exactly-once delivery mainly fit into one of these two categories: 1. The messsages where idempotent anyway or 2. They make an effort on receiver's side to deduplicate messages.

**** Reliable broadcast

It stands for /one sends, everyone receives/.

If a correct process delivers ~m~, then all correct processes deliver ~m~. The definition of /correct process/ will depend on the fault model we're based on. If the fault model is the crash model, then messages being lost is not an issue, and processes are correct if they don't crash. In the ommision model, we will have to add the condition of not all messages being lost.

For now, we're on the crash model.

#+BEGIN_SRC
   _            _            _      
  |A|          |B|          |C|    
   |            |            |           
   +_______     |            |     
   |       \--->+ ✓          |
   X            |            |
   X            |            |            
   v            v            v            
#+END_SRC

[Example 11] 

In this example, process ~A~ sends the first message of the braodcast to ~B~ and ~B~ delivers, but then ~A~ crashes before it can send it to ~C~.

As you can see, this protocol does not reliably deliver.

Let's say now that everytime a process receives a message, it broadcasts it to everyone else before delivering it.

#+BEGIN_SRC
   _            _            _      
  |A|          |B|          |C|    
   |            |            |           
   +_______     |            |     
   |       \--->+            |
   X          --+            |
   X<--------/  +--          |            
   X            |  \-------->+
   X            | ✓        --+
   X            |         / /+
   X            |<-------/ / | ✓
   X            |         /  |
   X<--------------------/   |
   X            |            |
   v            v            v            
#+END_SRC

[Example 12] 

Even thought ~A~ crashes, ~B~ will be also broadcasting the message to ~C~ (and ~A~) and then delivering it. When ~C~ receives the message it will do the same. 

So if ~B~ also crashes after the first message, then no one would have delivered, and the reliability condition would be met. If ~B~ crashes after sending the two messages, then ~C~ would be delivering but it would also be the only correct process and the condition would also be met.

A good addition would be to not send the message back to the sender.

If no one crashes, processes will see the message more than once. They will need to keep track of what they've delivered so they don't do it twice

If we need this protocol to also work on the omission model, we can just make each message follow the reliable deliver protocol as well, and build the broadcasting on top of it.

| Fault-tolerance often involves making copies.


* Replication

Why replicate data?

+ Fault-tolerance: prevent data loss.
+ Scale horizontally: divides the load.
+ Data locality

Downsides:
+ Higher cost
+ Keep consistent replicas


- Strong consistency :: A replicated storage system is strongly consistent if clients cannot tell that the data is replicated.

*** Primary-backup replication 
A particular process is chosen as the primary, and the rest are backup. Clients only operates with the primary

*** Chain replication 
The primary receives writes, sends the write event to next replica and this goes on until the last replica, that both applies the change and responds to client. Reads are from last replica. This one has better write throughput, and better at scaling horizontally, as it divides the load between reads and writes. Works best at 15% writes, 85% reads, as it divides the work more or less equally. Write latency is higher (depending on number of replicas), as replication requests are sequential.


*** Total order vs. determinism


#+BEGIN_SRC
   _            _          _            _
  |C1|         |R1|       |R2|         |C2|
   |            |          |            |
   |____x=1     |          |      _x=2__|
   |  \    \--->|          |<----/    / |
   |   \        |    x=1   |         /  |
   |    \----------------->|        /   |
   |            |          |   x=2 /    |
   |            |<----------------/     |
   |            |          |            |
   v            v          v            v
#+END_SRC

In the first replica, ~x~ will end up being 2, while in the second replica it will end up being 1. This is a violation of total order delivery.

If we had only one replica, ~x~ could end up being 1 or 2, but it does not violate total order delivery. So in this case the behaviour is not deterministic, as multiple runs of the same scenario could end up in different result.

** Consistency models

- Read your writes (RYW) :: Reading a value after wrinting it gives you the value just written. 

Example 13 shows a scenario where read your writes does not hold

#+BEGIN_SRC
  __            __           __
  |A|          |R1|         |R2|
   |            |            | 
   |\__x=4_     |            |
   |       \--->|            |
   |            |            |
   |  ack   ----|            |
   |<------/    |            |
   |            |            |
   |\___x?_     |            |
   |       \---------------->|
   |            |            |
   |  ???   -----------------|
   |<------/    |            |
   v            v            v
#+END_SRC
[Example 13]

- FIFO consistency :: Writes done by a process are seen by all processes.

The folowing example violates FIFO consistency.

#+BEGIN_SRC
   _            __           __           _
  |A|          |R1|         |R2|         |B|
   |            |            |            |  
   |\_dep50$    |            |            | 
   |       \--->|-           |            | 
   |            | \          |            | 
   |   ok   ----|  \dep50$   |            | 
   |<------/    |   \        |            | 
   |            |    \       |            | 
   |\_wd40$     |     \      |            | 
   |       \--->|- wd40\     |            | 
   |            | \--------->|  /---------| balance? 
   |  ok    ----|        \   |<-          |
   |<------/    |         \  |----------->| -40$
   |            |          ->|            | 
   v            v            v            v
#+END_SRC
[Example 14]

- Causal consistency :: Writes that are related by happens-before relationship must be seen in the same causal order by all processes.

The following example is a violation of causal consistency.

#+BEGIN_SRC
   _            __           __           _
  |A|          |R1|         |R2|         |B|
   |            |            |            |  
   |\dep100$    |            |           /| balance?
   |       \--->|            |          / | 
   |            |            |         /  | 
   |   ok   ----|            |        /   | 
   |<------/    |<-------------------/    | 
   |            |----------------         | 
   |            |            |   \------->| 100$ 
   |            |            |            | 
   |            |            |  /---------| wd50$ 
   |            |            |<-          |
   |            |            |----------->| No funds
   |            |            |            | 
   v            v            v            v
#+END_SRC
[Example 15]

#+begin_verse
Strong consistency \(\,\subset\,\) Causal consistency \(\, \subset\, \) FIFO consistency \( \,\subset\, \) RYW consistency 
#+end_verse

These are only some consistency models that can be find.


*** Strongly consistent replication models

There are two alreay discussed: primary back up and chain replication.
In these two models, there also has to exist some sort of coordinator that knows about replicas, the order(in the case of chain replication) and its status, whether it has failed or not. In case of failure, the coordinator will acknowledge that and rearrange the chain in the case of chain replication or the fact about who's the primary in primary back up.

But what if the coordinator fails? As we would like to keep coordination separated from the data nodes themselves, having no coordinator at all might not be the best solution. We could have many coordinators running a consensus protocol. 


* Consensus

When do we need consensus? When we have a bunch of processes and ...
- we need them to deliver messages in the same order (*totally-ordered broadcast*), or
- we need all of them to know about the rest (*group membership*), or
- one of them has a special role and the rest need to know about that (*leader election*), or
- there's a shared resource that can only be accesses by one at a time (*distributed mutual exclusion*), or
- they are participating in a transaction where a desicion about commiting/abouting needs to be agreed upon (*distributed transaction commit*)


#+BEGIN_SRC
         ┌───────────┐         
1 ──────►│           ├──────► 1
         │           │         
0 ──────►│ Consensus ├──────► 1
         │           │         
1 ──────►│           ├──────► 1
         └───────────┘         
#+END_SRC

Properties to /try/ to satisfy:

- Termination :: Each correct process eventually decides on a value.
- Agreement :: All correct processes decide on the same value.
- Validity (integrity, non-triviality) :: The agreed upon value must be one of the proposed values.

In the asynchronous network model and the crash fault model, no consensus algorithms can satisfy all these three. This was proved in 1983 by Fisher Lynch and Patterson (FLP)[fn:FLP], this is called the impossibility of distributed consensus with one faulty process.



** Paxos

Paxos[fn:paxos][fn:paxos-simple] is a consensus algorithm proposed by Leslie Lamport. 

In this algorithm, there's three roles that a process can play (one or more): 

- *Proposer* proposes values
- *Acceptor* contributes to choose from the proposed values
- *Learner* learns the agreed upon values

Paxos nodes must:
- persist data
- know which number conforms a majority of acceptors

The algorithm rules are:

*Phase 1* Kick off

- A *proposer* sends a prepare message, with a proposal id ~n~ to (at least) a majority of accepters. ~n~ must be unique and higher than any proposed number that this proposer has proposed before. Uniqueness must also work across proposers, so they will have to previously decide on a set of values each can use (if there's two proposers, one can take odd numbers and the other one even).

- When an *acceptor* receives a ~prepera(n)~ message, 
  - IF this node promised to ignore any proposal with this ~n~ id, it ignores the message.
  - IF NOT, it promises to ignore any request with a proposal number lower than ~n~, and replies with ~promise(n)~.  (*)

*Phase 2* Proposer has received ~promise(n)~ from a majority of acceptors (for some id ~n~).

- *Proposer* sends an ~accept(n, val)~ to (at least) a majority of acceptors, where ~n~ is the id promised in phase 1, and ~val~ is the actual value it wants to propose. (**)
- *Acceptor* on receiving an ~accept(n, val)~ will
  - IF this node promised to ignore any proposal with this ~n~, it ignores the message
  - IF NOT, replies with ~accepted(n,val)~ and also sends that message to all the learners

Let's look at a simple example:



#+BEGIN_SRC
             _            __           __           __           __            __
            |P|          |A1|         |A2|         |A3|         |L1|          |L2|            
             |            |             |            |            |             |   
  prepare(5) |\---------->|             |            |            |             |
             | \----------------------->|            |            |             | 
             |            |             |            |            |             | 
             |        ----|prom(5)      |            |            |             |
             |<------/  ----------------| prom(5)    |            |             | 
             |<------- /  |             |            |            |             |
             |            |             |            |            |             | 
             |            |             |            |            |             | 
milestone 1 ---------------------------------------------------------------------------
             |            |             |            |            |             |   
accept(5,X)  |\---------->|             |            |            |             |
             | \----------------------->|            |            |             | 
             |        ----|accptd(5,X)  |            |            |             |
             |<------/    |\            |            |            |             |
             |            |\--------------------------------------------------->|
             |            | ------------------------------------->|             |
             |            |             |            |            |             |   
             |          ----------------|accptd(5,X) |            |             |   
             |<--------/  |             |            |            |             |   
             |            |             |\------------------------------------->|
             |            |             |\----------------------->|             |
             |            |             |            |            |             | 
             |            |             |            |            |             | 
milestone 2 ---------------------------------------------------------------------------
             |            |             |            |            |             | 
             |            |             |            |            |             | 
             |            |             |            |            |             | 
             v            v             v            v            v             v
#+END_SRC
[Example 16]

First, the proposer ~P~ sends ~prepare(5)~ to a majority of accepters, in this case to ~A1~ and ~A2~. Both of them have never promised to reject ~5~, so they respond with ~promised(5)~.

At milestone 1, the proposed id ~n~ has been accepted by a majority of accepters.

Now ~P~ can propose a value to agree upon, in this case that value will be ~X~. For this, it sends ~accept(5, X)~ to a majority of acceptors. As ~A1~ and ~A2~ have never promised to reject ~5~, they respond with ~accepted(5,X)~, and they also send this message to learners.

At milestone 2, consensus is reached, as a majority af acceptors have accepted a value ~X~. ~A1~ and ~A2~ still don't know about this, but the proposer does.

Milestone 3 happens independently, when each node knows the consensus has been reached about the value ~X~.


So far we've seen the simple paxos. We've placed some asterisks in the algorithm description to elaborate more.

(*) In the case of phase 1, when an acceptor receives a message ~promise(n)~, and it has NOT promised to ignore than ~n~, then
  - IF it has previously accepted on something, it replies with ~promise(n, (n_prev, val_prev))~
  - IF NOT, it replies with ~promise(n)~

(**) In phase 2, when a proposer has received ~promise(n)~ or ~promise(n, (n_prev, val_prev))~ from a majority of acceptors (for some id ~n~), then it sends an ~accept(n, val)~ to (at least) a majority of acceptors, where ~n~ is the id promised in phase 1 and ~val~ is the  ~val_prev~ that came with the highest id ~n_prev~. If there where none of those messages, then it can propose its own value.

The example 16 does not explore these new paths, so let's look at another slighly more complex example, with two proposers. Learners are not drawn, for simplicity.


#+BEGIN_SRC
             _            __           __            __            __
            |P1|         |A1|         |A2|          |A3|          |P2|            
             |            |             |             |            |
  prepare(5) |\---------->|             |             |            |
             | \----------------------->|             |            |
             |            |             |             |            |
             |        ----|promise(5)   |             |            |
             |<------/  ----------------| prom(5)     |            |
             |<------- /  |             |             |            |
             |            |             |             |            |
             |            |             |             |            |
             |            |             |             |            |
accept(5,X)  |\---------->|             |             |            |
             | \----------------------->|             |<----------/| prepare(6)
             |        ----|acceptd(5,X) |             |\         / |
             |<------/    |<----------------------------\-------/  |
             |            |             |             |  \-------->| promise(6)
             |            |             |             |       /    |             
             |          ----------------|acceptd(5,X) |      /     |             
             |<--------/  |             |             |     /      |             
             |            |             |<-----------------/       |   
             |            |             |             |            |
             |            |\-------------------------------------->| promise(6,(5,X))
             |            |             |\------------------------>| promise(6,(5,X))
             |            |             |             |            |      
             |            |             |             |            |         
             |            |             |             |<----------/| accept(6, X)
             |            |             |<-----------------------/ |
             |            |             |             |\---------->| accepted(6,X)
             |            |             |\------------------------>| accepted(6,X)
             v            v             v            v            v      
#+END_SRC
[Example 17]

In this example, after ~A1~ and ~A2~ have responded with ~accepted(5,X)~ and ~P1~ has reached milestone 1, proposer ~P2~ starts a new consensus run by sending ~prepare(6)~ messages to the three accepters. ~A3~ hadn't promissed or accepted anuthing before so it responds with ~promise(6)~.

In the case of ~A1~ and ~A2~, they had both already accepted the value ~X~ for run ~5~, so they respond with ~promise(6,(5,X)))~. When ~P2~ receives these messages, it takes the value of the higher previously promised ~n~. In this case they are both 5, so the value chosen will be ~X~.

At this point ~P2~ starts phase 2 by sending ~accept(6, X)~ to a majority of accepters (~A1~, ~A2~ and ~A3~) which respond with ~accepted~ messages.


*Facts about paxos*
- If a majority of acceptors promise on a number ~n~, no paxos run with value less than ~n~ can succeed.
- If a majority of acceptors accept on ~(n, value)~ then consensus is reached. Consensus is about ~value~ an not id ~n~.
- If a proposer or a learner gets majority of accepts on a value for a specific ~n~, then they know that consensus has been reached on that value.

*Dueling proposers*

What about the FLP result? Well, the property that paxos fails to guarantee is termination. In example 18 you can see how two proposers can step into each others toes.


#+BEGIN_SRC
             _            __             __            __            __
            |P1|         |A1|           |A2|          |A3|          |P2|            
             |            |               |             |            |
  prepare(5) |\---------->|               |             |            |
             | \------------------------->|             |            |
             |            |               |             |            |
             |        ----|               |             |            |
  promise(5) |<------/  ------------------|             |            |
  promise(5) |<------- /  |               |             |            |
             |            |               |             |            |
             |            |               |             |<----------/| prepare(6)
             |            |               |             |\         / |
             |            |               |<--------------\-------/  |
             |            |               |             |  \-------->| promise(6)
             |            |               |             |            |     
             |            |               |\------------------------>| promise(6)
  accept(5,A)|\---------->|               |             |            |
             | \------------------------->X             |            |
      .      |            |               |             |            |
      .      |        ----|accepted(5,A)  |             |            |
      .      |<------/    |               |             |            |
   TIMEOUT   |            |               |             |            |
  prepare(7) |\---------->|               |             |<----------/| accept(6,B)
             | \------------------------->|             |\         / |
             |        ----|               |             | \       /  |
  promise(7) |<------/  ------------------|             |  \-----/-->| accepted(6,B)
  promise(7) |<------- /  |               |             |       /    |     .
             |            |               X<-------------------/     |     . 
             |            |               |             |            |     .
             |            |               |             |            |  TIMEOUT
             |            |               |             |<----------/| prepare(8)
             |            |               |             |\         / |
             |            |               |<--------------\-------/  |
             |            |               |             |  \-------->| promise(8)
             |            |               |             |            |     
             |            |               |\------------------------>| promise(8)
  accept(7,A)|\---------->|               |             |            |
             | \------------------------->X             |            |
             |            |               |             |            |
             v            v               v             v            v      
#+END_SRC
[Example 18]

In the example, ~P1~ starts a consensus run, gets promises on ~5~ from ~A1~ and ~A2~.\\
Right after that, and before ~P1~ can propose value ~A~, ~P2~ starts a consensus run and gets promises on ~6~ from ~A2~ and ~A3~. This can happen because ~A2~ still did not accept anything.\\
When ~P1~ sends the accept messages, ~A1~ will respond accepting, but message will be ignored by ~A2~, because ~A2~ has just promised on ignoring ids less than ~6~.\\
Time passes and ~P1~ timeouts on waiting for accepted mesage from ~A2~, so it starts a new round. It sends prepare message with an increased id of ~7~, and gets the promises back from ~A1~ and ~A2~, as before. \\
Right after ~A2~ promises on rejecting ids less than ~7~, the accept message from proposer ~P2~ arrived, and therefore ignored. So ~P2~ will also timeout waiting for ~A2~'s response.\\
This can go on and on forever.

So how can we deal with this issue?

We can try to mitigate the problem using some heuristics.

One possibility is to /try/ to have only one proposer, so dueling never happens. For that, we can elect a leader, using another consensus algorithm. Why another? to use one that guarantees termination. Maybe that other algorithm does not guarantee uniqueness of the leader, but even in that case it reduces the chances of dueling happening, as long as it decides on only one leader most of the time.

Another alternative is imposing an exponential backoff pause after timeout, so that eventually one proposer can get its value accepted before the other process proposes a new id.


**** Multi-paxos

What if we neeed to decide on many values? 
For example, let's say we're deciding on totally order delivery, so each time a message arrives, a consensus run needs to take place. 
An alternative version of the algorithm lets the proposer that won a run on paxos to keep proposing values under the winner id, skipping phase 1. As soon as a new proposers starts a new consensus run under a higher id, that privilege will be taken off him.
But as long as noone else interrupts the sequence, phase 2 can be used.


**** Fault tolerance of paxos

As long as a majority of acceptors do not crash, paxos can go on.
So we want to tolerate f crashes, the we will need 2*f+1 acceptors.

Regarding proposers, we need at least always one.

On the other hand, it works ok under ommision faults.


**** Alternatives to paxos 

- Raft (Diego Ongaro & John Ousterhout, 2014) - Inteded to be easy to understand
- Zab (Zookeeper atomic broadcast) - Yahoo! research
- ViewStamped replication (Brian Oki & Barbara Liskov, 1998)






* Dynamo style DBs

** Passive vs Active replication

Active (State machine replication): Execute each operation in each replica. So what is sent over the network is the operation itself. This is better if the state update is large (multiply each entry by 10).

Passive: Execute the operation in the primary and send updated state to each replica. This is better if the operation is expensive.



** CAP

CAP stands for: Consistency, Availability, Partition tolerance

+ Network partitions :: When a subset of machines cannot talk to another subset.

+ Availability :: Every request receives a response

Primary backup replication chooses a sync replication, where responses to clients are given after receiveing the acknowdlege of each replica. In the case of a netwwork partition, when the client can talk to the primary but primary cannot talk to any replica, the primary won't respond to the client, or respond with an error, compromising availability but protecting consistency.

Dynamo style DBs chooses a different approach. They prioritize availability, while introducing the chance of breaking consistency.

The CAP theorem talks about this. 

+ CAP theorem :: In a scenario where partitions can happen, you cannot have perfect availability and perfect consistency.


** Eventual consistency and strong convergence

Replicas eventually agree, if clients stop sending queries. [Liveness property]

Consistency guarantees are safety properties (FIFO, Strong, RYW, Causal)

There is a safety property than can somehow express strong consistency, and is *Strong convergence*: Replicas that have delivered the same set of updates, have the equivalent state.


#+BEGIN_SRC
a) _            _          _            _
  |C1|         |R1|       |R2|         |C2|
   |            |          |            |
   |____x=1     |          |      _x=2__|
   |  \    \--->|          |<----/    / |
   |   \        |    x=1   |         /  |
   |    \----------------->|        /   |
   |            |          |   x=2 /    |
   |            |<----------------/     |
   |            |          |            |
   v            v          v            v
              {x=2}      {x=1}

b) _            _          _            _
  |C1|         |R1|       |R2|         |C2|
   |            |          |            |
   |____x=1     |          |      _y=2__|
   |  \    \--->|          |<----/    / |
   |   \        |    x=1   |         /  |
   |    \----------------->|        /   |
   |            |          |   y=2 /    |
   |            |<----------------/     |
   |            |          |            |
   v            v          v            v
            {x=1,y=2}  {x=1,y=2}
#+END_SRC
[Example 19]

Example 19.a violates both strong consistence and strong convergence, while example 19.b only violates strong consistency, while strong convergence is satisfied.

"Strong eventual consistency" is sometimes used to refer to strong convergence + eventual consistency. (A liveness + a safety property)

In the following example we see a possible solution for the issue

#+BEGIN_SRC
  |C1|         |R1|       |R2|         |C2|
   |            |          |            |
   |____x=1     |          |      _x=2__|
   |  \    \--->|          |<----/    / |
   |   \        |    x=1   |         /  |
   |    \----------------->|        /   |
   |            |          |   x=2 /    |
   |            |<----------------/     |
   |            |          |            |
   v            v          v            v
            {x=1,2}  {x=2,1}
#+END_SRC
[Example 20]

The replicas will look at the vector clocks of the two updates and realize that they were concurrent, and store both values, so that clients can decide later wich value to keep. By doing this we now have strong convergence.




** Dealing with replicas that disagree

These two concepts are synonims in general, but in the context of the dynamo paper, they mean slightly two different things.

- Anti-entropy :: Resolving conflict in applicatation state (KVS = Application).
- Gossip :: Resolving conflict in view state, meaning, in knowing who's up, who's alive. 

The main difference here is the load. Anti-entropy has to hanle much higher load, as it has to share the whole KV state in each round. For this, it uses merkle trees (aka hash trees).

** Quorum consistency

How many replica responses should clients wait for?

Quorum system let you configure this:

- N :: Number of replicas
- W :: Write quorum
- R :: Read quorum

If we have this setting: N=3, W=3, R=1, it might look similar to primary-backup or chain replication, but the important difference is that in those moedls, clients only talk to one designated replica, while here clients can talk to anyone. So strong consistency is not totally guaranteed.

Dynamo proposes something like: N=3, W=2, R=2. More generally W+R>N, so we know that any read quqrum will intersect with any write quorum.

** Tail latency

Latency at high end of the distribution

* Sharding

What if data does not fit in one machine? Splitting the dataset among multiple nodes is know as /sharding/ or /paritioning/ (not to be confused with network partition).

How do we decide how to split the dataset among the shards? What makes a sharding strategy good?

We would like to avoid hotspots, anddd threfore evenly distribute the load.

+ Partition by key range :: One option is to partition by key. So if keys are string, then shards would be similar to a volume of an encyclopedia. Problem with this is that we end up with an uneven distribution of data (Unless you know it's going to be evenly distributed). 

+ Partition by hash of keys, mod N :: An improved approach involves hashing the keys in order to evenly distribute the key set across machines. So the hashed values can be distributed using ~mod N~ function, where ~N~ is the number of machines. The problem with this approach is that if a machine goes down, or a new machine needs to be added, then the ~N~ changes, and many values will need to be moved to a different machine.

Ideally, if we had ~K~ keys and ~N~ nodes, then ~K/N~ would be the minimum movement possible, taking some keys out of the old machines and move them to the new one. 

+ Consistent hasing :: Let's arrange the hashed key set around a ring, and let's assign machines to a partition of that ring. When a desicion has to happen, the key to be inserted will first be hashed to a value in the ring and the chosen machine will be the one closest, following the ring clockwise.

#+BEGIN_SRC
             0
        _.---+---._
      .'           '. M1
     /             8 \
  M4| 50              |
48  +                 +  16 
    |                 | 
     \            22 /
      '. 38        .'
      M3'-._____.-' M2
             +
             32

M1 stores [50, 8)
M2 stores [8, 22)
M3 stores [22, 38)
M4 stores [38, 50)

#+END_SRC

So in this example, if the key ~k1~ wants to be written, and let's say ~hash(k1)=14~, it will be stored at machine ~M2~

Now let's say a new node needs to be added, and it's added at position 62. The keys that will have to be moved are 
~[50, 62)~, from ~M1~ to the new node ~M5~.

In the case a node crash, its keys will have to be moved to the subsecuent node in the ring. This is why dynamo DB handles replication also using the ring, having a master node for a particular piece of data and choosing backups to the the following nodes in the ring.

+ Consistent hashing with virtual nodes :: A drawback of the consistent hashing algorithm we just described is that the load might not be evenly distributed. This might now always be the case, and it should improve when more and more data is store. Having said this, an improvent would be to hash each machine to many different places in the ring, so that chances of even distribution of the load is higher.

#+BEGIN_SRC
             M4   M3
    M2  _.---+---._
      .'           '. M1
     /               \
  M4|                 |
    +                 +  M2
    |                 | 
   M4\               / M1
      '.           .'
      M3'-._____.-' M2
          M1    M3
#+END_SRC


By using virtual nodes, we can also decide how many virtual nodes each real machine should have, so that load can be sort of customized per machine. So in the case of machine M1 having more space than the rest, we can assign more virtual nodes to it.

* MapReduce

A broad categorization of systems is

+ Online systems :: Services, that wait for clients to make requests, and try to respond as quickly as possible. Low latency and availability is prioritized.

+ Offline systems :: Batch processing systems, where high throuput is often prioritized.

MapReduce comes to help this last group. *MapReduce* is a tool for computing /derived data/. Raw data is the authoritative version of data, the format new data comes in, while derived data is the result of taking this existing data and transforming it somehow.

*** Example: Inverted index.
We can use  MapReduce to compute inverted indexes, as the ones used on search engines. If we have

| Document | Words                              |
|----------+------------------------------------|
|        1 | The, quick, brown, fox, jumps, ... |
|        2 | The, dog, growls                   |
|        3 | My, dog,                           |

And convert it into

| Word   | Document |
|--------+----------|
| The    |     1, 2 |
| quick  |        1 |
| brown  |        1 |
| fox    |        1 |
| jumps  |        1 |
| dog    |     2, 3 |
| growls |        2 |
| My     |        2 |


This computation can be divided into two phases, the /map/ phase, that will convert

(1, [The, quick, brown, fox, jumps]), (2, [The, dog, growls])

into

[(The, 1), (quick, 1), ...], [(The, 2), (dog, 2), ...], [(dog, 3), ...]

And finally a /reduce/ phase that will combine the list of pairs into the final form.

But what is the ral challenge then? Well, if the input is huge, we might want to split the computation into several different machines, parallelizing it. MapReduce structure turns out to be convenient for paralellization.

1. *MAP*: The first phase, the map phase, is completely parallelizable. Each document can be processed by a different machine independently. The results will be stored in each machine local storage.
2. *SHUFFLE*: Then, we need to split the load to each reducer. The default way is to just ~hash(key) mod N~ where ~N~ is the number of reducers. The reduce will now need to read from each local storage to get the intermediate results.
3. *REDUCE*: The reducer will run and write the output to a distributed file system (GFS)

Of course the only thing the programmer needs to care about is the functions provided to the map and reduce phases (and maybe shuffle if they want).

#+begin_src 
map : (K,V) -> [(K', V')]
reduce : (K', [V']) -> [V']
#+end_src

* Footnotes

[fn:ytlink] [[https://www.youtube.com/playlist?list=PLNPUF5QyWU8PydLG2cIJrCvnn5I_exhYx][CSE138 - YouTube playlist]]

[fn:chandy-lamport] [[https://decomposition.al/blog/2019/04/26/an-example-run-of-the-chandy-lamport-snapshot-algorithm/][Lindsey Kuper's blog post on Chandy-Lamport algorithm]]


[fn:FLP] [[https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf][Impossibility of Distributed Consensus with One Faulty Process]]

[fn:paxos] Leslie Lamport. The part-time parliament. ACM Transactions on Computer Systems, 16(2):133–169, 1998.

[fn:paxos-simple] [[https://courses.cs.vt.edu/~cs5204/fall08-kafura/Papers/FaultTolerance/Paxos-Simple-Lamport.pdf][Leslie Lamport. Paxos Made Simple. 2021.]]
